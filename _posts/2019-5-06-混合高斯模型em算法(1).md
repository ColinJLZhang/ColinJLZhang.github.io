---
layout:     post
title:      混合高斯模型EM算法（1）
subtitle:   高斯混合模型中的隐含变量
date:       2019-05-06
author:     Colin
header-img: img/post-GMM.png
catalog: true
comments: true
tags:
    - EM算法
    - 概率论
    - GMM（混合高斯模型）
---
### EM算法的核心本质：
    - expectation：当前参数下隐含变量的参数的期望值（完全数据下的对数似然函数）
    - maximization： 最大化上述期望

### 混合模型中的隐含变量理解：

在看过了很多篇博客对混合高斯模型的EM算法讲解之后，我始终有一个地方感觉有点学艺不精，而大家的博客很少提到这一点，在下实在愚钝经过一番理解后记录下来以免下次遇到同样的问题。

混合高斯模型是一个极为经典的用多个分布拟合某个随机变量的值的方法，这里是用多个高斯模型混合，定义如下：

$$
P(y | \theta)=\sum_{i=1}^{K} \alpha_{k} \phi\left(y | \theta_{k}\right)
$$

其中，$\alpha_k$是系数，$\sum_{k=1}^K\alpha_k=1$, $\phi(y | \theta_{k})$是高斯密度函数，$\theta_{k}=(\mu_{k}, \sigma_{k}^{2})$,

$$
\phi\left(y | \theta_{\lambda}\right)=\frac{1}{\sqrt{2 \pi} \sigma_{k}} \exp \left(-\frac{\left(y-\mu_{k}\right)^{2}}{2 \sigma_{k}^{2}}\right)
$$

那么如何从概率学的角度来理解GMM呢？（假设我们的观测数据是y）  
我们不妨假设这里有三个混合高斯模型，$\alpha_1$,$\alpha_2$,$\alpha_3$是选择这三个模型的概率，即y有$\alpha_1$的概率是由第1个高斯模型产生（y是第一个高斯模型的自变量x）,y有$\alpha_2$的概率是由第2个高斯模型产生,y有$\alpha_3$的概率是由第3个高斯模型产生,所以在整个混合模型中产生观测数据y的概率是将所有的可能相加即：

$$
P(y | \theta) = \alpha_{1} \phi\left(y | \theta_{1}\right) + \alpha_{2} \phi\left(y | \theta_{2}\right) + \alpha_{3} \phi\left(y | \theta_{3}\right)
$$

*用概率学的思维思考问题！*

<a href="https://blog.csdn.net/wenglican3523/article/details/79130690">这里引用一个博客的理解:</a> 
    
> 使用EM算法必须明确隐变量。求解GMM的时候设想观测数据x是这样产生的：首选依赖GMM的某个高斯分量的系数概率（因为系数取值在0~1之间，因此可以看做是一个概率取值）选择到这个高斯分量，然后根据这个被选择的高斯分量生成观测数据。然后隐变量就是某个高斯分量是否被选中：选中就为1，否则为0。
> 
<br>按照这样的设想：隐变量是一个向量，并且这个向量中只有一个元素取值为1，其它的都是0。因为假设只有一个高斯分量被选中并产生观测数据。然而我们的GMM的一个观测数据在直观上应该是每个高斯分量都有产生，而不是由一个高斯分量单独生成，只是重要性不同（由系数控制）。那么，这样的隐变量假设合理吗？
>
<br>答案是合理，只是理解起来比较“费劲”而已。
>
<br>首先明确一点：GMM的观测数据是啥，GMM的函数结果又是啥。如果是一个一维的GMM，那么其观测数据就是任意一个实数。而GMM这个概率密度函数在输入这个观测数据之后输出的是这个实数被GMM产生的概率而已。
>
<br>接着，现在我们不知道GMM具体的参数值，想要根据观测数据去求解其参数。而GMM的参数是由各个高斯分量的参数再加上权值系数组成的。那么我们就先假定，如果这个观测值只是由其中一个高斯分量产生，去求解其中一个高斯分量的参数。我们假设不同的观测值都有一个产生自己的唯一归宿，就像K-means算法一样。然后在后面的迭代过程中，根据数据整体似然函数的优化过程，逐渐找到一个最优的分配方案。然而，不同于K-means算法的是，我们最终给出的只是某一个观测是由某一个高斯分量唯一生成的概率值，而不是确定下来的属于某一类。每个高斯分量其实都可以产生这个观测数据只是输出不同而已，即产生观测数据的概率不同。最后，根据每个高斯分量产生观测数据的可能性不同，结合其权值汇总出整个GMM产生这个观测数据的概率值。

为了表示观测变量y来自于某一个高斯模型我们定义一个0-1的K维随机变量${\gamma}$

$$
\gamma_{j k}=\left\{\begin{array}{l}{1}\qquad\qquad观测值j来自于第k个模型 \\ {0}\qquad\qquad其他\end{array}\right.
$$

那么现在的完全数据是（观测数据+未观测数据）：

$$
\left(y_{j}, \gamma_{j 1}, \gamma_{j 2}, \cdots, \gamma_{j K}\right), \quad j=1,2, \cdots, N
$$

### EM算法求解GMM：
#### 1.完全数据的似然函数：

$$
\begin{aligned}
P(y, \gamma | \theta)&=\prod_{j=1} P\left(y_{j}, \gamma_{j 1}, \gamma_{j 2}, \cdots, \gamma_{j K} | \theta\right)\\
&=\prod_{k=1}^{K} \prod_{j=1}^{N}\left[\alpha_{k} \phi\left(y, | \theta_{k}\right)\right]^{\gamma_{\mu}}\\
&=\prod_{k=1}^{K} \alpha_{k}^{n_{k}} \prod_{j=1}^{N}\left[\phi\left(y_{j} | \theta_{k}\right)\right]^{\gamma_{\mu}}\\
&=\prod_{k=1}^{K} \alpha_{k}^{n_{k}} \prod_{j=1}^{N}\left[\frac{1}{\sqrt{2 \pi} \sigma_{k}} \exp \left(-\frac{\left(y_{j}-\mu_{k}\right)^{2}}{2 \sigma_{k}^{2}}\right)\right]^{\gamma_{\mu}}
\end{aligned}
$$

其中，$n_{k}=\sum_{j=1}^{N} \gamma_{j k}, \sum_{k=1}^{K} n_{k}=N$,对数似然函数为：

$$
\log P(y, \gamma | \theta)=\sum_{k=1}^{K}\left\{n_{k} \log \alpha_{k}+\sum_{j=1}^{N} \gamma_{j k}\left[\log \left(\frac{1}{\sqrt{2 \pi}}\right)-\log \sigma_{k}-\frac{1}{2 \sigma_{k}^{2}}\left(y_{j}-\mu_{k}\right)^{2}\right]\right\}
$$

#### 2.EM算法的E步，确定Q函数：  

$$
Q\left(\theta, \theta^{(i)}\right)=E\left[\log P(y, \gamma | \theta) | y, \theta^{(n)}\right]
$$

Q函数是对完全数据对数似然函数的隐含变量的期望，我们可以把完全对数似然函数看作一个以隐含变量作为自变量的函数，这里的隐含变量是一个随机变量，根据随机变量期望的性质我们可以进一步的变换这个Q函数：
<center>  
    <img src="..\..\..\..\img\article\meanandfunction.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图1 随机变量期望与函数关系 《概率论与数理统计》</div>
</center>

$$
\begin{aligned}
Q\left(\theta, \theta^{(i)}\right)&=E\left\{\sum_{k=1}^{K}\left\{n_{k} \log \alpha_{k}+\sum_{i=1}^{N} \gamma_{\mu}\left[\log \left(\frac{1}{\sqrt{2 \pi}}\right)-\log \sigma_{k}-\frac{1}{2 \sigma_{k}^{2}}\left(y_{j}-\mu_{k}\right)^{2}\right]\right\}\right\}\\
&=\sum_{k=1}^{K}\left\{\sum_{j=1}^{N}\left(E \gamma_{j k}\right) \log \alpha_{k}+\sum_{j=1}^{J_{N}}\left(E \gamma_{j k}\right)\left[\log \left(\frac{1}{\sqrt{2 \pi}}\right)-\log \sigma_{k}-\frac{1}{2 \sigma_{k}^{2}}\left(y_{j}-\mu_{k}\right)^{2}\right]\right\}
\end{aligned}
$$

这里需要计算隐含变量的期望 $E(\gamma_{j k}|y, \theta)$，记为$\hat{\gamma}_{jk}$,

$$
\begin{aligned}
\hat{\gamma}_{j k}&=E\left(\gamma_{j k} | y, \theta\right)=P\left(\gamma_{j k}=1 | y, \theta\right)\\
&=\frac{P\left(\gamma_{j k}=1, y, | \theta\right)}{\sum_{k=1}^{K} P\left(\gamma_{j k}=1, y_{j} | \theta\right)}
\\
&=\frac{P\left(y_{j} | \gamma_{j k}=1, \theta\right) P\left(\gamma_{j k}=1 | \theta\right)}{\sum_{k=1}^{K} P\left(y_{j} | \gamma_{j k}=1, \theta\right) P\left(\gamma_{j k}=1 | \theta\right)}\\
&=\frac{\alpha_{k} \phi\left(y, | \theta_{k}\right)}{\sum_{k=1}^{K} \alpha_{k} \phi\left(y_{j} | \theta_{k}\right)}, \quad j=1,2, \cdots, N ; k=1,2, \cdots, K
\end{aligned}
$$  

带入前述Q函数，

$$
Q\left(\theta, \theta^{(i)}\right)=\sum_{k=1}^{K}\left\{n_{k} \log \alpha_{k}+\sum_{\mathbf{j}=1}^{N} \hat{\gamma}_{j k}\left[\log \left(\frac{1}{\sqrt{2 \pi}}\right)-\log \sigma_{k}-\frac{1}{2 \sigma_{k}^{2}}\left(y_{j}-\mu_{k}\right)^{2}\right]\right\} \quad(9.29)
$$

#### 3.EM算法的M步,更新模型参数：

完成了Q函数的建立，下一步的任务就是在每一次的迭代过程中极大化Q:

$$
\theta^{(i+1)}=\arg \max _{\theta} Q\left(\theta, \theta^{(i)}\right)
$$

求出$\hat\mu_k$, $\hat\sigma_k^2$ 对Q函数的偏导数并将其等于零，即可得到当前迭代下的最大值。$\hat\alpha_k$ 是在$\sum_{k=1}^K\alpha_k=1$条件下求偏导得到的。

$$
\hat{\mu}_{k}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k} y_{j}}{\sum_{j=1}^{N} \hat{\gamma}_{j k}}, \quad k=1,2, \cdots, K
$$

$$
\hat{\sigma}_{k}^{2}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}\left(y_{j}-\mu_{k}\right)^{2}}{\sum_{j=1}^{N} \hat{\gamma}_{j k}}, \quad k=1,2, \cdots, K
$$

$$
\hat{\alpha}_{k}=\frac{n_{k}}{N}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}}{N}, \quad k=1,2, \cdots, K
$$

#### 4.EM算法流程总结
<center>  
    <img src="..\..\..\..\img\article\em-algorithm.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图2 EM算法步骤 《统计学习方法》</div>
</center>
#### 5.代码部分：
实验结果：
<center>  
    <img src="..\..\..\..\img\article\EM.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图3 EM算法拟合混合高斯模型</div>
</center>

可以看到，第九次迭代几乎和第六次迭代是重合的说明了算法的收敛性，但是注意这个算法对初始值十分敏感，不恰当的初始值甚至可能会出现无穷大的数而导致无法计算。

python代码：
```python
import os
import numpy as np
import matplotlib.pyplot as plt


def gaussian(m, s, y):
    return 1 / (np.sqrt(2 * np.pi) * s) * np.exp(-(y - m) ** 2 / (2 * s ** 2))


def initialize():
    # classes
    k = 3
    # [[m1,s1],[m2,s2],[m3,s3]]
    theta = np.array([[20, 1], [10, 1], [-20, 1]])
    # mix coefficient
    alpha = np.array([0.3, 0.3, 0.4])
    param = (k, theta, alpha)
    return param


def expectation(param, y):
    k, theta, alpha = param
    # resp.shape = (y,k)
    resp = []
    for i in range(len(y)):
        yiResp = []
        msum = np.dot(alpha, gaussian(theta[:, 0], theta[:, 1], y[i]))
        for ki in range(k):
            yiResp.append(alpha[ki] * gaussian(theta[ki, 0], theta[ki, 1], y[i]) / msum)
        resp.append(yiResp)
    return np.array(resp)


def maximization(resp, param, y):
    k, thetai_1, _ = param
    thetai = np.zeros((k, 2))
    alphai = np.zeros((k,))
    for ki in range(k):
        mi = np.dot(resp[:, ki], y) / sum(resp[:, ki])
        si = np.sqrt(np.dot(resp[:, ki], (y - thetai_1[ki, 0]) ** 2) / sum(resp[:, ki]))
        alpha = sum(resp[:, ki]) / len(y)
        thetai[ki, :] = mi, si
        alphai[ki] = alpha
    param = (k, thetai, alphai)
    return param


if __name__ == '__main__':
    x = np.arange(-100, 100, 5)
    m1 = 40
    m2 = 0
    m3 = -40
    s1 = 5
    s2 = 4
    s3 = 5
    # plt.plot(x, gaussian(m1, s1, x))
    # plt.plot(x, gaussian(m2, s2, x))
    # plt.plot(x, gaussian(m3, s3, x))
    _y = 0.2 * gaussian(m1, s1, x) + 0.6 * gaussian(m2, s2, x) + 0.2 * gaussian(m3, s3, x)
    plt.plot(x, _y, 'k*')
    p = _y / sum(_y)
    y = [np.random.choice(x, p=p.ravel()) for j in range(len(x))]
    param = initialize()
    x = np.arange(-100, 100, 0.1)
    for i in range(10):
        resp = expectation(param, y)
        param = maximization(resp, param, y)
        if i % 3 == 0:
            k, theta, alpha = param
            plt.plot(x,
                     alpha[0] * gaussian(theta[0, 0], theta[0, 1], x) + alpha[1] * gaussian(theta[1, 0], theta[1, 1],
                                                                                            x) +
                     alpha[2] * gaussian(theta[2, 0], theta[2, 1], x), label="%dth iteration" % (i,))
    print(theta)
    print(alpha)
    plt.legend(loc='upper right')
    plt.show()

```
————  
参考：  
盛骤 《概率论与数理统计》 第四版  
李航 《统计学习方法》


**HAPPY CODING!**